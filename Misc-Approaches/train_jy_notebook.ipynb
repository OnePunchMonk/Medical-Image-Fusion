{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Hyperparameters\n",
    "    batch_size = 4\n",
    "    image_size = 256\n",
    "    num_epochs = 10\n",
    "    learning_rate = 1e-4\n",
    "\n",
    "    # Create dummy data for CT and MRI images (B, 1, H, W)\n",
    "    ct_images = torch.randn(batch_size, 1, image_size, image_size)\n",
    "    mri_images = torch.randn(batch_size, 1, image_size, image_size)\n",
    "    # For demonstration, assume the target fused image is the average of CT and MRI images\n",
    "    target_fused = (ct_images + mri_images) / 2.0\n",
    "\n",
    "    # Instantiate models\n",
    "    model = KaleidoFusionNet(in_channels=1, embed_dim=64, latent_dim=64, base_filters=64)\n",
    "    discriminator = Discriminator(in_channels=1, base_filters=64)\n",
    "\n",
    "    # Optimizers\n",
    "    gen_optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    dis_optimizer = torch.optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        gen_optimizer.zero_grad()\n",
    "        dis_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the fusion model\n",
    "        fused_output = model(ct_images, mri_images)  # (B, 1, H, W)\n",
    "\n",
    "        # Reconstruction loss (L1 loss) between the generated output and target fused image\n",
    "        rec_loss = F.l1_loss(fused_output, target_fused)\n",
    "        \n",
    "        # Adversarial loss for generator: aim to fool the discriminator\n",
    "        pred_fake = discriminator(fused_output)\n",
    "        valid_labels = torch.ones_like(pred_fake)\n",
    "        adv_loss = F.binary_cross_entropy_with_logits(pred_fake, valid_labels)\n",
    "\n",
    "        # Total generator loss (weighted sum)\n",
    "        gen_loss = rec_loss + 0.001 * adv_loss\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        # Update discriminator: classify real fused images vs. generated ones\n",
    "        dis_optimizer.zero_grad()\n",
    "        # For real images (target fused), label as 1\n",
    "        pred_real = discriminator(target_fused)\n",
    "        loss_real = F.binary_cross_entropy_with_logits(pred_real, valid_labels)\n",
    "        # For fake images, label as 0\n",
    "        pred_fake = discriminator(fused_output.detach())\n",
    "        fake_labels = torch.zeros_like(pred_fake)\n",
    "        loss_fake = F.binary_cross_entropy_with_logits(pred_fake, fake_labels)\n",
    "        dis_loss = (loss_real + loss_fake) / 2\n",
    "        dis_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Gen Loss: {gen_loss.item():.4f}, Dis Loss: {dis_loss.item():.4f}\")\n",
    "\n",
    "    # Example inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        fused_result = model(ct_images, mri_images)\n",
    "        print(\"Fused result shape:\", fused_result.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_dir DATA_DIR]\n",
      "                             [--base_filters BASE_FILTERS]\n",
      "                             [--loss_type {l1,l2,mse}]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--num_epochs NUM_EPOCHS]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--num_workers NUM_WORKERS]\n",
      "                             [--save_interval SAVE_INTERVAL]\n",
      "                             [--output_dir OUTPUT_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\yuvra\\AppData\\Roaming\\jupyter\\runtime\\kernel-v3b1cca6da8e020ee975dd0994a4f0f9d80677dcc4.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuvra\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from model import MultiModalFusionModel, get_loss_function\n",
    "from dataset import MultiModalDataset\n",
    "\n",
    "def train(args):\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = MultiModalFusionModel(in_channels=1, base_filters=args.base_filters)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    train_dataset = MultiModalDataset(\n",
    "        data_dir=args.data_dir,\n",
    "        split='train',\n",
    "        transform=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = MultiModalDataset(\n",
    "        data_dir=args.data_dir,\n",
    "        split='val',\n",
    "        transform=False\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = get_loss_function(args.loss_type)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(args.num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{args.num_epochs} [Train]\") as pbar:\n",
    "            for batch_idx, (ct_images, mri_images, target_images) in enumerate(pbar):\n",
    "                # Move data to device\n",
    "                ct_images = ct_images.to(device)\n",
    "                mri_images = mri_images.to(device)\n",
    "                target_images = target_images.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(ct_images, mri_images)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, target_images)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update progress bar\n",
    "                epoch_loss += loss.item()\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with tqdm(val_loader, desc=f\"Epoch {epoch+1}/{args.num_epochs} [Val]\") as pbar:\n",
    "                for batch_idx, (ct_images, mri_images, target_images) in enumerate(pbar):\n",
    "                    # Move data to device\n",
    "                    ct_images = ct_images.to(device)\n",
    "                    mri_images = mri_images.to(device)\n",
    "                    target_images = target_images.to(device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(ct_images, mri_images)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = criterion(outputs, target_images)\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    val_loss += loss.item()\n",
    "                    pbar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{args.num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, os.path.join(args.output_dir, 'best_model.pth'))\n",
    "            print(f\"Saved best model with validation loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % args.save_interval == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, os.path.join(args.output_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(args.output_dir, 'loss_curve.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Train Multi-Modal Fusion Model')\n",
    "    \n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--data_dir', type=str, default='./data', help='Path to dataset directory')\n",
    "    \n",
    "    # Model parameters\n",
    "    parser.add_argument('--base_filters', type=int, default=64, help='Number of base filters in the model')\n",
    "    parser.add_argument('--loss_type', type=str, default='l1', choices=['l1', 'l2', 'mse'], help='Loss function type')\n",
    "    \n",
    "    # Training parameters\n",
    "    parser.add_argument('--batch_size', type=int, default=8, help='Batch size for training')\n",
    "    parser.add_argument('--num_epochs', type=int, default=5, help='Number of training epochs')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001, help='Initial learning rate')\n",
    "    parser.add_argument('--num_workers', type=int, default=4, help='Number of data loading workers')\n",
    "    parser.add_argument('--save_interval', type=int, default=10, help='Epoch interval to save checkpoints')\n",
    "    parser.add_argument('--output_dir', type=str, default='./output', help='Directory to save outputs')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    train(args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aggar\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\aggar\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]: 100%|██████████| 16/16 [01:27<00:00,  5.49s/it, loss=0.676]\n",
      "Epoch 1/5 [Val]: 100%|██████████| 4/4 [00:14<00:00,  3.68s/it, loss=0.797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 0.9984, Val Loss: 0.7805\n",
      "Saved best model with validation loss: 0.7805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|██████████| 16/16 [01:49<00:00,  6.82s/it, loss=0.517]\n",
      "Epoch 2/5 [Val]: 100%|██████████| 4/4 [00:14<00:00,  3.67s/it, loss=0.705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Train Loss: 0.5775, Val Loss: 0.6893\n",
      "Saved best model with validation loss: 0.6893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Train]: 100%|██████████| 16/16 [01:36<00:00,  6.05s/it, loss=0.444]\n",
      "Epoch 3/5 [Val]: 100%|██████████| 4/4 [00:13<00:00,  3.48s/it, loss=0.441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Train Loss: 0.4755, Val Loss: 0.4620\n",
      "Saved best model with validation loss: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Train]: 100%|██████████| 16/16 [01:23<00:00,  5.19s/it, loss=0.429]\n",
      "Epoch 4/5 [Val]: 100%|██████████| 4/4 [00:13<00:00,  3.46s/it, loss=0.421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Train Loss: 0.4354, Val Loss: 0.4388\n",
      "Saved best model with validation loss: 0.4388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Train]: 100%|██████████| 16/16 [01:23<00:00,  5.21s/it, loss=0.396]\n",
      "Epoch 5/5 [Val]: 100%|██████████| 4/4 [00:17<00:00,  4.45s/it, loss=0.403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Train Loss: 0.4254, Val Loss: 0.4244\n",
      "Saved best model with validation loss: 0.4244\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ================================\n",
    "# Model Components: KaleidoFusionNet\n",
    "# ================================\n",
    "\n",
    "#############################################\n",
    "# 1. Vision Transformer (ViT) Encoder Block #\n",
    "#############################################\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Splits the input image into patches and embeds them.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, embed_dim, patch_size=16):\n",
    "        super(PatchEmbed, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        x = self.proj(x)  # (B, embed_dim, H/patch_size, W/patch_size)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.flatten(2)  # (B, embed_dim, num_patches)\n",
    "        x = x.transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified Vision Transformer encoder that splits the image into patches,\n",
    "    adds positional embeddings, and applies transformer encoder layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, embed_dim=64, patch_size=16, num_layers=6, num_heads=4, dropout=0.1):\n",
    "        super(ViTEncoder, self).__init__()\n",
    "        self.patch_embed = PatchEmbed(in_channels, embed_dim, patch_size)\n",
    "        # For simplicity, assume fixed image size 256x256 -> num_patches = (256//patch_size)^2\n",
    "        num_patches = (256 // patch_size) ** 2  \n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        x = self.patch_embed(x)  # (B, num_patches, embed_dim)\n",
    "        B, N, C = x.shape\n",
    "        # If number of patches differs from assumed, interpolate positional embeddings\n",
    "        if N != self.pos_embed.shape[1]:\n",
    "            pos_embed = F.interpolate(self.pos_embed.transpose(1,2), size=N, mode='linear', align_corners=False).transpose(1,2)\n",
    "        else:\n",
    "            pos_embed = self.pos_embed\n",
    "        x = x + pos_embed\n",
    "        x = self.transformer(x)  # (B, num_patches, embed_dim)\n",
    "        # Reshape tokens back into a feature map (assume square layout)\n",
    "        h = w = int(N ** 0.5)\n",
    "        x = x.transpose(1,2).reshape(B, C, h, w)\n",
    "        return x\n",
    "\n",
    "####################################\n",
    "# 2. VAE Block for Latent Mapping  #\n",
    "####################################\n",
    "\n",
    "class VAEBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects encoder features into a latent space using a VAE formulation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, latent_dim):\n",
    "        super(VAEBlock, self).__init__()\n",
    "        self.fc_mu = nn.Conv2d(in_channels, latent_dim, kernel_size=1)\n",
    "        self.fc_logvar = nn.Conv2d(in_channels, latent_dim, kernel_size=1)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "######################################################\n",
    "# 3. Cross-Modal Attention Fusion (Feature Fusion)   #\n",
    "######################################################\n",
    "\n",
    "class CrossModalAttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Fuses latent representations from CT and MRI using multi-head attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim):\n",
    "        super(CrossModalAttentionFusion, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=latent_dim, num_heads=4, batch_first=True)\n",
    "    \n",
    "    def forward(self, z_ct, z_mri):\n",
    "        # Flatten spatial dimensions: (B, latent_dim, H, W) -> (B, N, latent_dim)\n",
    "        B, C, H, W = z_ct.shape\n",
    "        z_ct_flat = z_ct.flatten(2).transpose(1, 2)   # (B, N, C)\n",
    "        z_mri_flat = z_mri.flatten(2).transpose(1, 2)   # (B, N, C)\n",
    "        # Use CT latent as query and MRI latent as key/value (or vice versa)\n",
    "        fused, _ = self.attention(z_ct_flat, z_mri_flat, z_mri_flat)\n",
    "        # Reshape back to (B, latent_dim, H, W)\n",
    "        fused = fused.transpose(1,2).reshape(B, C, H, W)\n",
    "        return fused\n",
    "\n",
    "#############################################\n",
    "# 4. GAN-Based Decoder (Generator) for Output #\n",
    "#############################################\n",
    "\n",
    "class GANDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decodes the fused latent representation into a fused image using transposed convolutions.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, base_filters=64):\n",
    "        super(GANDecoder, self).__init__()\n",
    "        self.up1 = nn.ConvTranspose2d(latent_dim, base_filters*8, kernel_size=2, stride=2)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(base_filters*8, base_filters*8, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_filters*8),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.up2 = nn.ConvTranspose2d(base_filters*8, base_filters*4, kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(base_filters*4, base_filters*4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_filters*4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.up3 = nn.ConvTranspose2d(base_filters*4, base_filters*2, kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(base_filters*2, base_filters*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_filters*2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.up4 = nn.ConvTranspose2d(base_filters*2, base_filters, kernel_size=2, stride=2)\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(base_filters, base_filters, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_filters),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.final_conv = nn.Conv2d(base_filters, 1, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.up1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.up3(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.up4(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "#########################################\n",
    "# 5. Diffusion Refinement Module        #\n",
    "#########################################\n",
    "\n",
    "class DiffusionRefinement(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies iterative refinement (a simplified diffusion-like process) to the generated image.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, num_steps=3):\n",
    "        super(DiffusionRefinement, self).__init__()\n",
    "        self.num_steps = num_steps\n",
    "        self.refinement_block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for _ in range(self.num_steps):\n",
    "            residual = self.refinement_block(x)\n",
    "            x = x + residual\n",
    "        return x\n",
    "\n",
    "#########################################\n",
    "# 6. KaleidoFusionNet: The Complete Model#\n",
    "#########################################\n",
    "\n",
    "class KaleidoFusionNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete multi-modal fusion model.\n",
    "    \n",
    "    Input: ct_image and mri_image (each with shape: (B, 1, H, W))\n",
    "    Output: fused image with the same input/output interface.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, embed_dim=64, latent_dim=64, base_filters=64):\n",
    "        super(KaleidoFusionNet, self).__init__()\n",
    "        # Dual-stream ViT encoders for CT and MRI images\n",
    "        self.ct_encoder = ViTEncoder(in_channels=in_channels, embed_dim=embed_dim)\n",
    "        self.mri_encoder = ViTEncoder(in_channels=in_channels, embed_dim=embed_dim)\n",
    "        \n",
    "        # VAE blocks to map encoder features to a shared latent space\n",
    "        self.ct_vae = VAEBlock(in_channels=embed_dim, latent_dim=latent_dim)\n",
    "        self.mri_vae = VAEBlock(in_channels=embed_dim, latent_dim=latent_dim)\n",
    "        \n",
    "        # Cross-modal attention fusion module\n",
    "        self.fusion = CrossModalAttentionFusion(latent_dim=latent_dim)\n",
    "        \n",
    "        # GAN-based decoder (Generator)\n",
    "        self.decoder = GANDecoder(latent_dim=latent_dim, base_filters=base_filters)\n",
    "        \n",
    "        # Diffusion refinement module to further improve output quality\n",
    "        self.diffusion = DiffusionRefinement(channels=1)\n",
    "    \n",
    "    def forward(self, ct_image, mri_image):\n",
    "        # Encode CT and MRI images\n",
    "        ct_features = self.ct_encoder(ct_image)    # (B, embed_dim, H', W')\n",
    "        mri_features = self.mri_encoder(mri_image)    # (B, embed_dim, H', W')\n",
    "        \n",
    "        # Project features into latent space using VAE blocks\n",
    "        z_ct, mu_ct, logvar_ct = self.ct_vae(ct_features)   # (B, latent_dim, H', W')\n",
    "        z_mri, mu_mri, logvar_mri = self.mri_vae(mri_features)  # (B, latent_dim, H', W')\n",
    "        \n",
    "        # Fuse the latent representations using cross-modal attention\n",
    "        fused_latent = self.fusion(z_ct, z_mri)   # (B, latent_dim, H', W')\n",
    "        \n",
    "        # Decode fused latent representation via the GAN-based decoder\n",
    "        gen_image = self.decoder(fused_latent)    # (B, 1, H_out, W_out)\n",
    "        \n",
    "        # Apply diffusion refinement to further enhance the output image\n",
    "        refined_image = self.diffusion(gen_image)\n",
    "        \n",
    "        return refined_image\n",
    "\n",
    "#########################################\n",
    "# Optional: Get Loss Function           #\n",
    "#########################################\n",
    "\n",
    "def get_loss_function(loss_type='l1'):\n",
    "    \"\"\"\n",
    "    Return the specified loss function.\n",
    "    \"\"\"\n",
    "    if loss_type.lower() == 'l1':\n",
    "        return nn.L1Loss()\n",
    "    elif loss_type.lower() in ['l2', 'mse']:\n",
    "        return nn.MSELoss()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss type: {loss_type}\")\n",
    "\n",
    "# ======================================\n",
    "# Training Script\n",
    "# ======================================\n",
    "\n",
    "# (Assuming you have a MultiModalDataset defined in dataset.py)\n",
    "from dataset import MultiModalDataset  # Ensure this file exists and implements your dataset\n",
    "\n",
    "def train(args):\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = KaleidoFusionNet(in_channels=1, embed_dim=64, latent_dim=64, base_filters=args.base_filters)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    train_dataset = MultiModalDataset(\n",
    "        data_dir=args.data_dir,\n",
    "        split='train',\n",
    "        transform=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = MultiModalDataset(\n",
    "        data_dir=args.data_dir,\n",
    "        split='val',\n",
    "        transform=False\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = get_loss_function(args.loss_type)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(args.num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{args.num_epochs} [Train]\") as pbar:\n",
    "            for batch_idx, (ct_images, mri_images, target_images) in enumerate(pbar):\n",
    "                # Move data to device\n",
    "                ct_images = ct_images.to(device)\n",
    "                mri_images = mri_images.to(device)\n",
    "                target_images = target_images.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(ct_images, mri_images)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, target_images)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with tqdm(val_loader, desc=f\"Epoch {epoch+1}/{args.num_epochs} [Val]\") as pbar:\n",
    "                for batch_idx, (ct_images, mri_images, target_images) in enumerate(pbar):\n",
    "                    ct_images = ct_images.to(device)\n",
    "                    mri_images = mri_images.to(device)\n",
    "                    target_images = target_images.to(device)\n",
    "                    \n",
    "                    outputs = model(ct_images, mri_images)\n",
    "                    loss = criterion(outputs, target_images)\n",
    "                    val_loss += loss.item()\n",
    "                    pbar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{args.num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, os.path.join(args.output_dir, 'best_model.pth'))\n",
    "            print(f\"Saved best model with validation loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint every save_interval epochs\n",
    "        if (epoch + 1) % args.save_interval == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, os.path.join(args.output_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(args.output_dir, 'loss_curve.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Train KaleidoFusionNet for Multi-Modal Fusion')\n",
    "    \n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--data_dir', type=str, default='./data', help='Path to dataset directory')\n",
    "    \n",
    "    # Model parameters\n",
    "    parser.add_argument('--base_filters', type=int, default=64, help='Number of base filters in the model')\n",
    "    parser.add_argument('--loss_type', type=str, default='l1', choices=['l1', 'l2', 'mse'], help='Loss function type')\n",
    "    \n",
    "    # Training parameters\n",
    "    parser.add_argument('--batch_size', type=int, default=8, help='Batch size for training')\n",
    "    parser.add_argument('--num_epochs', type=int, default=5, help='Number of training epochs')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001, help='Initial learning rate')\n",
    "    parser.add_argument('--num_workers', type=int, default=4, help='Number of data loading workers')\n",
    "    parser.add_argument('--save_interval', type=int, default=10, help='Epoch interval to save checkpoints')\n",
    "    parser.add_argument('--output_dir', type=str, default='./output', help='Directory to save outputs')\n",
    "    \n",
    "    args, unknown = parser.parse_known_args()\n",
    "    train(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aggar\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from epoch 4 with validation loss 0.4244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:29<00:00,  7.47s/it, L1=0.436, L2=0.451]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "L1 Loss: 0.4414\n",
      "L2 Loss: 0.4477\n",
      "PSNR: 11.7819 dB\n",
      "SSIM: 0.0757\n",
      "Evaluation completed! Results saved to ./evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Import the correct model class\n",
    "# from model import KaleidoFusionNet\n",
    "from dataset import MultiModalDataset\n",
    "\n",
    "def evaluate(args):\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create model (KaleidoFusionNet)\n",
    "    model = KaleidoFusionNet(in_channels=1, embed_dim=64, latent_dim=64, base_filters=args.base_filters)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(args.checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded checkpoint from epoch {checkpoint['epoch']} with validation loss {checkpoint['val_loss']:.4f}\")\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    test_dataset = MultiModalDataset(\n",
    "        data_dir=args.data_dir,\n",
    "        split='test',\n",
    "        transform=False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    l1_loss = nn.L1Loss()\n",
    "    l2_loss = nn.MSELoss()\n",
    "    \n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    total_l1_loss = 0.0\n",
    "    total_l2_loss = 0.0\n",
    "    total_psnr = 0.0\n",
    "    total_ssim = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_loader, desc=\"Evaluating\") as pbar:\n",
    "            for batch_idx, (ct_images, mri_images, target_images) in enumerate(pbar):\n",
    "                # Move data to device\n",
    "                ct_images = ct_images.to(device)\n",
    "                mri_images = mri_images.to(device)\n",
    "                target_images = target_images.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(ct_images, mri_images)\n",
    "                \n",
    "                # Calculate losses\n",
    "                batch_l1_loss = l1_loss(outputs, target_images).item()\n",
    "                batch_l2_loss = l2_loss(outputs, target_images).item()\n",
    "                \n",
    "                # Calculate PSNR and SSIM for each sample in the batch\n",
    "                for i in range(outputs.size(0)):\n",
    "                    # Convert to numpy arrays for PSNR and SSIM calculation\n",
    "                    output_np = outputs[i, 0].cpu().numpy()\n",
    "                    target_np = target_images[i, 0].cpu().numpy()\n",
    "                    \n",
    "                    # Normalize to [0, 1]\n",
    "                    output_np = (output_np - output_np.min()) / (output_np.max() - output_np.min() + 1e-8)\n",
    "                    target_np = (target_np - target_np.min()) / (target_np.max() - target_np.min() + 1e-8)\n",
    "                    \n",
    "                    # Calculate PSNR and SSIM\n",
    "                    batch_psnr = psnr(target_np, output_np, data_range=1.0)\n",
    "                    batch_ssim = ssim(target_np, output_np, data_range=1.0)\n",
    "                    \n",
    "                    total_psnr += batch_psnr\n",
    "                    total_ssim += batch_ssim\n",
    "                \n",
    "                total_l1_loss += batch_l1_loss\n",
    "                total_l2_loss += batch_l2_loss\n",
    "                pbar.set_postfix(L1=batch_l1_loss, L2=batch_l2_loss)\n",
    "                \n",
    "                # Save sample images\n",
    "                if batch_idx < args.num_samples_to_save:\n",
    "                    for i in range(min(outputs.size(0), 4)):  # Save up to 4 images per batch\n",
    "                        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "                        \n",
    "                        ct_img = ct_images[i, 0].cpu().numpy()\n",
    "                        mri_img = mri_images[i, 0].cpu().numpy()\n",
    "                        target_img = target_images[i, 0].cpu().numpy()\n",
    "                        output_img = outputs[i, 0].cpu().numpy()\n",
    "                        \n",
    "                        axes[0].imshow(ct_img, cmap='gray')\n",
    "                        axes[0].set_title('CT Image')\n",
    "                        axes[0].axis('off')\n",
    "                        \n",
    "                        axes[1].imshow(mri_img, cmap='gray')\n",
    "                        axes[1].set_title('MRI Image')\n",
    "                        axes[1].axis('off')\n",
    "                        \n",
    "                        axes[2].imshow(target_img, cmap='gray')\n",
    "                        axes[2].set_title('Target Image')\n",
    "                        axes[2].axis('off')\n",
    "                        \n",
    "                        axes[3].imshow(output_img, cmap='gray')\n",
    "                        axes[3].set_title('Fused Image (Output)')\n",
    "                        axes[3].axis('off')\n",
    "                        \n",
    "                        plt.tight_layout()\n",
    "                        plt.savefig(os.path.join(args.output_dir, f'sample_{batch_idx}_{i}.png'))\n",
    "                        plt.close()\n",
    "    \n",
    "    num_samples = len(test_dataset)\n",
    "    avg_l1_loss = total_l1_loss / len(test_loader)\n",
    "    avg_l2_loss = total_l2_loss / len(test_loader)\n",
    "    avg_psnr = total_psnr / num_samples\n",
    "    avg_ssim = total_ssim / num_samples\n",
    "    \n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"L1 Loss: {avg_l1_loss:.4f}\")\n",
    "    print(f\"L2 Loss: {avg_l2_loss:.4f}\")\n",
    "    print(f\"PSNR: {avg_psnr:.4f} dB\")\n",
    "    print(f\"SSIM: {avg_ssim:.4f}\")\n",
    "    \n",
    "    with open(os.path.join(args.output_dir, 'evaluation_results.txt'), 'w') as f:\n",
    "        f.write(f\"Evaluation Results:\\n\")\n",
    "        f.write(f\"L1 Loss: {avg_l1_loss:.4f}\\n\")\n",
    "        f.write(f\"L2 Loss: {avg_l2_loss:.4f}\\n\")\n",
    "        f.write(f\"PSNR: {avg_psnr:.4f} dB\\n\")\n",
    "        f.write(f\"SSIM: {avg_ssim:.4f}\\n\")\n",
    "    \n",
    "    print(f\"Evaluation completed! Results saved to {args.output_dir}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Evaluate Multi-Modal Fusion Model')\n",
    "    \n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--data_dir', type=str, default='./data', help='Path to dataset directory')\n",
    "    \n",
    "    # Model parameters\n",
    "    parser.add_argument('--base_filters', type=int, default=64, help='Number of base filters in the model')\n",
    "    # parser.add_argument('--checkpoint_path', type=str, required=True, help='Path to model checkpoint')\n",
    "\n",
    "    parser.add_argument('--checkpoint_path', type=str, default=r\"C:\\Users\\aggar\\Downloads\\Telegram Desktop\\DL_updated_11march\\DL\\output\\best_model.pth\", help='Path to model checkpoint')\n",
    "\n",
    "\n",
    "    \n",
    "    # Evaluation parameters\n",
    "    parser.add_argument('--batch_size', type=int, default=8, help='Batch size for evaluation')\n",
    "    parser.add_argument('--num_workers', type=int, default=4, help='Number of data loading workers')\n",
    "    parser.add_argument('--output_dir', type=str, default='./evaluation', help='Directory to save outputs')\n",
    "    parser.add_argument('--num_samples_to_save', type=int, default=5, help='Number of sample batches to save')\n",
    "    \n",
    "    # Use parse_known_args() to ignore unknown arguments passed by Jupyter\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    evaluate(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
