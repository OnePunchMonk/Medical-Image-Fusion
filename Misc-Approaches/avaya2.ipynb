{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Basic convolutional block with batch normalization and ReLU activation.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder network for a single modality (CT or MRI).\"\"\"\n",
    "    def __init__(self, in_channels=1, base_filters=64):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Downsampling path\n",
    "        self.enc1 = ConvBlock(in_channels, base_filters)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.enc2 = ConvBlock(base_filters, base_filters*2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.enc3 = ConvBlock(base_filters*2, base_filters*4)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.enc4 = ConvBlock(base_filters*4, base_filters*8)\n",
    "        \n",
    "        # Store intermediate features for skip connections\n",
    "        self.features = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.features = []\n",
    "        \n",
    "        # Encoder path with feature storage\n",
    "        x1 = self.enc1(x)\n",
    "        self.features.append(x1)\n",
    "        x = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.enc2(x)\n",
    "        self.features.append(x2)\n",
    "        x = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.enc3(x)\n",
    "        self.features.append(x3)\n",
    "        x = self.pool3(x3)\n",
    "        \n",
    "        x4 = self.enc4(x)\n",
    "        self.features.append(x4)\n",
    "        \n",
    "        return x4, self.features\n",
    "\n",
    "class FusionModule(nn.Module):\n",
    "    \"\"\"Fusion module to combine features from CT and MRI encoders.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(FusionModule, self).__init__()\n",
    "        \n",
    "        # Fusion convolution layer\n",
    "        self.fusion_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels*2, in_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, ct_features, mri_features):\n",
    "        # Concatenate features along channel dimension\n",
    "        fused_features = torch.cat([ct_features, mri_features], dim=1)\n",
    "        # Apply fusion convolution\n",
    "        fused_features = self.fusion_conv(fused_features)\n",
    "        return fused_features\n",
    "\n",
    "class KolmogorovArnoldModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Kolmogorov–Arnold-inspired module.\n",
    "    This module approximates the idea that a multivariate function can be represented\n",
    "    as a sum of univariate functions (applied via simple 1x1 convolutions here).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, num_terms=3):\n",
    "        super(KolmogorovArnoldModule, self).__init__()\n",
    "        self.num_terms = num_terms\n",
    "        # 'psi' transforms: project each channel to a scalar (via 1x1 conv)\n",
    "        self.psi = nn.ModuleList([nn.Conv2d(in_channels, 1, kernel_size=1) for _ in range(num_terms)])\n",
    "        # 'phi' transforms: project back from scalar to original channel dimension\n",
    "        self.phi = nn.ModuleList([nn.Conv2d(1, in_channels, kernel_size=1) for _ in range(num_terms)])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = 0\n",
    "        # Apply each pair of psi and phi transforms and sum the results.\n",
    "        for i in range(self.num_terms):\n",
    "            term = self.psi[i](x)\n",
    "            term = self.relu(term)\n",
    "            term = self.phi[i](term)\n",
    "            out = out + term\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder network (pseudo-sensing module) to reconstruct the fused image.\"\"\"\n",
    "    def __init__(self, in_channels, base_filters=64):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Upsampling path\n",
    "        self.upconv3 = nn.ConvTranspose2d(in_channels, base_filters*4, kernel_size=2, stride=2)\n",
    "        self.dec3 = ConvBlock(base_filters*4, base_filters*4)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(base_filters*4, base_filters*2, kernel_size=2, stride=2)\n",
    "        self.dec2 = ConvBlock(base_filters*2, base_filters*2)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(base_filters*2, base_filters, kernel_size=2, stride=2)\n",
    "        self.dec1 = ConvBlock(base_filters, base_filters)\n",
    "        \n",
    "        # Final output layer\n",
    "        self.final_conv = nn.Conv2d(base_filters, 1, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Decoder path\n",
    "        x = self.upconv3(x)\n",
    "        x = self.dec3(x)\n",
    "        \n",
    "        x = self.upconv2(x)\n",
    "        x = self.dec2(x)\n",
    "        \n",
    "        x = self.upconv1(x)\n",
    "        x = self.dec1(x)\n",
    "        \n",
    "        # Final convolution to get output image\n",
    "        x = self.final_conv(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MultiModalFusionModel(nn.Module):\n",
    "    \"\"\"Complete multi-modal fusion model with dual-stream encoders, fusion module, a Kolmogorov–Arnold-inspired module, and decoder.\"\"\"\n",
    "    def __init__(self, in_channels=1, base_filters=64):\n",
    "        super(MultiModalFusionModel, self).__init__()\n",
    "        \n",
    "        # Dual-stream encoders for CT and MRI\n",
    "        self.ct_encoder = Encoder(in_channels, base_filters)\n",
    "        self.mri_encoder = Encoder(in_channels, base_filters)\n",
    "        \n",
    "        # Fusion module\n",
    "        self.fusion = FusionModule(base_filters*8, base_filters*8)\n",
    "        \n",
    "        # Kolmogorov–Arnold-inspired module to further process the fused features\n",
    "        self.kam_module = KolmogorovArnoldModule(base_filters*8, num_terms=3)\n",
    "        \n",
    "        # Decoder (pseudo-sensing module)\n",
    "        self.decoder = Decoder(base_filters*8, base_filters)\n",
    "    \n",
    "    def forward(self, ct_image, mri_image):\n",
    "        # Encode CT and MRI images\n",
    "        ct_features, ct_skip_features = self.ct_encoder(ct_image)\n",
    "        mri_features, mri_skip_features = self.mri_encoder(mri_image)\n",
    "        \n",
    "        # Fuse features from both modalities\n",
    "        fused_features = self.fusion(ct_features, mri_features)\n",
    "        # Apply the Kolmogorov–Arnold-inspired module\n",
    "        fused_features = self.kam_module(fused_features)\n",
    "        \n",
    "        # Decode fused features to generate the output image\n",
    "        output = self.decoder(fused_features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "def get_loss_function(loss_type='ssim_l1'):\n",
    "    \"\"\"Return the specified loss function.\"\"\"\n",
    "    \n",
    "    if loss_type.lower() == 'l1':\n",
    "        return nn.L1Loss()\n",
    "    \n",
    "    elif loss_type.lower() in ['l2', 'mse']:\n",
    "        return nn.MSELoss()\n",
    "    \n",
    "    elif loss_type.lower() == 'ssim_l1':\n",
    "        class SSIML1Loss(nn.Module):\n",
    "            def __init__(self, alpha=0.1, beta=0.9):  # More focus on SSIM\n",
    "                super(SSIML1Loss, self).__init__()\n",
    "                self.alpha = alpha  # L1 weight\n",
    "                self.beta = beta    # SSIM weight\n",
    "                self.l1_loss = nn.L1Loss()\n",
    "\n",
    "            def forward(self, output, target):\n",
    "                l1_loss = self.l1_loss(output, target)\n",
    "                ssim_loss = 1 - pytorch_msssim.ssim(output, target, data_range=1.0)\n",
    "                return self.alpha * l1_loss + self.beta * ssim_loss\n",
    "        \n",
    "        return SSIML1Loss()\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss type: {loss_type}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "# from model import MultiModalFusionModel, get_loss_function\n",
    "from dataset import MultiModalDataset\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = MultiModalFusionModel(in_channels=1, base_filters=args.base_filters)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    train_dataset = MultiModalDataset(\n",
    "        data_dir=args.data_dir,\n",
    "        split='train',\n",
    "        transform=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = MultiModalDataset(\n",
    "        data_dir=args.data_dir,\n",
    "        split='val',\n",
    "        transform=False\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = get_loss_function(args.loss_type)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(args.num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{args.num_epochs} [Train]\") as pbar:\n",
    "            for batch_idx, (ct_images, mri_images, target_images) in enumerate(pbar):\n",
    "                # Move data to device\n",
    "                ct_images = ct_images.to(device)\n",
    "                mri_images = mri_images.to(device)\n",
    "                target_images = target_images.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(ct_images, mri_images)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, target_images)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update progress bar\n",
    "                epoch_loss += loss.item()\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with tqdm(val_loader, desc=f\"Epoch {epoch+1}/{args.num_epochs} [Val]\") as pbar:\n",
    "                for batch_idx, (ct_images, mri_images, target_images) in enumerate(pbar):\n",
    "                    # Move data to device\n",
    "                    ct_images = ct_images.to(device)\n",
    "                    mri_images = mri_images.to(device)\n",
    "                    target_images = target_images.to(device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(ct_images, mri_images)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = criterion(outputs, target_images)\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    val_loss += loss.item()\n",
    "                    pbar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{args.num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, os.path.join(args.output_dir, 'best_model.pth'))\n",
    "            print(f\"Saved best model with validation loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % args.save_interval == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, os.path.join(args.output_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(args.output_dir, 'loss_curve.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Train Multi-Modal Fusion Model')\n",
    "    \n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--data_dir', type=str, default='./data', help='Path to dataset directory')\n",
    "    \n",
    "    # Model parameters\n",
    "    parser.add_argument('--base_filters', type=int, default=64, help='Number of base filters in the model')\n",
    "    parser.add_argument('--loss_type', type=str, default='l1', choices=['l1', 'l2', 'mse'], help='Loss function type')\n",
    "    \n",
    "    # Training parameters\n",
    "    parser.add_argument('--batch_size', type=int, default=8, help='Batch size for training')\n",
    "    parser.add_argument('--num_epochs', type=int, default=5, help='Number of training epochs')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001, help='Initial learning rate')\n",
    "    parser.add_argument('--num_workers', type=int, default=4, help='Number of data loading workers')\n",
    "    parser.add_argument('--save_interval', type=int, default=10, help='Epoch interval to save checkpoints')\n",
    "    parser.add_argument('--output_dir', type=str, default='./output', help='Directory to save outputs')\n",
    "    \n",
    "    args,unknown = parser.parse_known_args()\n",
    "    train(args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded checkpoint from epoch 64 with validation loss 0.0560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [01:07<00:00, 16.95s/it, L1=0.0572, L2=0.0164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "L1 Loss: 0.0590\n",
      "L2 Loss: 0.0178\n",
      "PSNR: 20.8359 dB\n",
      "SSIM: 0.3959\n",
      "Evaluation completed! Results saved to ./evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Import the correct model class\n",
    "# from model import KaleidoFusionNet\n",
    "from dataset import MultiModalDataset\n",
    "\n",
    "def evaluate(args):\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create model (KaleidoFusionNet)\n",
    "    # model = KaleidoFusionNet(in_channels=1, embed_dim=64, latent_dim=64, base_filters=args.base_filters)\n",
    "\n",
    "    model = MultiModalFusionModel(in_channels=1, base_filters=args.base_filters)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # model = model.to(device)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(args.checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded checkpoint from epoch {checkpoint['epoch']} with validation loss {checkpoint['val_loss']:.4f}\")\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    test_dataset = MultiModalDataset(\n",
    "        data_dir=args.data_dir,\n",
    "        split='test',\n",
    "        transform=False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    l1_loss = nn.L1Loss()\n",
    "    l2_loss = nn.MSELoss()\n",
    "    \n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    total_l1_loss = 0.0\n",
    "    total_l2_loss = 0.0\n",
    "    total_psnr = 0.0\n",
    "    total_ssim = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_loader, desc=\"Evaluating\") as pbar:\n",
    "            for batch_idx, (ct_images, mri_images, target_images) in enumerate(pbar):\n",
    "                # Move data to device\n",
    "                ct_images = ct_images.to(device)\n",
    "                mri_images = mri_images.to(device)\n",
    "                target_images = target_images.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(ct_images, mri_images)\n",
    "                \n",
    "                # Calculate losses\n",
    "                batch_l1_loss = l1_loss(outputs, target_images).item()\n",
    "                batch_l2_loss = l2_loss(outputs, target_images).item()\n",
    "                \n",
    "                # Calculate PSNR and SSIM for each sample in the batch\n",
    "                for i in range(outputs.size(0)):\n",
    "                    # Convert to numpy arrays for PSNR and SSIM calculation\n",
    "                    output_np = outputs[i, 0].cpu().numpy()\n",
    "                    target_np = target_images[i, 0].cpu().numpy()\n",
    "                    \n",
    "                    # Normalize to [0, 1]\n",
    "                    output_np = (output_np - output_np.min()) / (output_np.max() - output_np.min() + 1e-8)\n",
    "                    target_np = (target_np - target_np.min()) / (target_np.max() - target_np.min() + 1e-8)\n",
    "                    \n",
    "                    # Calculate PSNR and SSIM\n",
    "                    batch_psnr = psnr(target_np, output_np, data_range=1.0)\n",
    "                    batch_ssim = ssim(target_np, output_np, data_range=1.0)\n",
    "                    \n",
    "                    total_psnr += batch_psnr\n",
    "                    total_ssim += batch_ssim\n",
    "                \n",
    "                total_l1_loss += batch_l1_loss\n",
    "                total_l2_loss += batch_l2_loss\n",
    "                pbar.set_postfix(L1=batch_l1_loss, L2=batch_l2_loss)\n",
    "                \n",
    "                # Save sample images\n",
    "                if batch_idx < args.num_samples_to_save:\n",
    "                    for i in range(min(outputs.size(0), 4)):  # Save up to 4 images per batch\n",
    "                        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "                        \n",
    "                        ct_img = ct_images[i, 0].cpu().numpy()\n",
    "                        mri_img = mri_images[i, 0].cpu().numpy()\n",
    "                        target_img = target_images[i, 0].cpu().numpy()\n",
    "                        output_img = outputs[i, 0].cpu().numpy()\n",
    "                        \n",
    "                        axes[0].imshow(ct_img, cmap='gray')\n",
    "                        axes[0].set_title('CT Image')\n",
    "                        axes[0].axis('off')\n",
    "                        \n",
    "                        axes[1].imshow(mri_img, cmap='gray')\n",
    "                        axes[1].set_title('MRI Image')\n",
    "                        axes[1].axis('off')\n",
    "                        \n",
    "                        axes[2].imshow(target_img, cmap='gray')\n",
    "                        axes[2].set_title('Target Image')\n",
    "                        axes[2].axis('off')\n",
    "                        \n",
    "                        axes[3].imshow(output_img, cmap='gray')\n",
    "                        axes[3].set_title('Fused Image (Output)')\n",
    "                        axes[3].axis('off')\n",
    "                        \n",
    "                        plt.tight_layout()\n",
    "                        plt.savefig(os.path.join(args.output_dir, f'sample_{batch_idx}_{i}.png'))\n",
    "                        plt.close()\n",
    "    \n",
    "    num_samples = len(test_dataset)\n",
    "    avg_l1_loss = total_l1_loss / len(test_loader)\n",
    "    avg_l2_loss = total_l2_loss / len(test_loader)\n",
    "    avg_psnr = total_psnr / num_samples\n",
    "    avg_ssim = total_ssim / num_samples\n",
    "    \n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"L1 Loss: {avg_l1_loss:.4f}\")\n",
    "    print(f\"L2 Loss: {avg_l2_loss:.4f}\")\n",
    "    print(f\"PSNR: {avg_psnr:.4f} dB\")\n",
    "    print(f\"SSIM: {avg_ssim:.4f}\")\n",
    "    \n",
    "    with open(os.path.join(args.output_dir, 'evaluation_results.txt'), 'w') as f:\n",
    "        f.write(f\"Evaluation Results:\\n\")\n",
    "        f.write(f\"L1 Loss: {avg_l1_loss:.4f}\\n\")\n",
    "        f.write(f\"L2 Loss: {avg_l2_loss:.4f}\\n\")\n",
    "        f.write(f\"PSNR: {avg_psnr:.4f} dB\\n\")\n",
    "        f.write(f\"SSIM: {avg_ssim:.4f}\\n\")\n",
    "    \n",
    "    print(f\"Evaluation completed! Results saved to {args.output_dir}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Evaluate Multi-Modal Fusion Model')\n",
    "    \n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--data_dir', type=str, default='./data', help='Path to dataset directory')\n",
    "    \n",
    "    # Model parameters\n",
    "    parser.add_argument('--base_filters', type=int, default=64, help='Number of base filters in the model')\n",
    "    # parser.add_argument('--checkpoint_path', type=str, required=True, help='Path to model checkpoint')\n",
    "\n",
    "    parser.add_argument('--checkpoint_path', type=str, default=r\"C:\\Users\\aggar\\Downloads\\Telegram Desktop\\DL_updated_11march-with2new\\DL\\output\\best_model.pth\", help='Path to model checkpoint')\n",
    "\n",
    "\n",
    "    \n",
    "    # Evaluation parameters\n",
    "    parser.add_argument('--batch_size', type=int, default=8, help='Batch size for evaluation')\n",
    "    parser.add_argument('--num_workers', type=int, default=4, help='Number of data loading workers')\n",
    "    parser.add_argument('--output_dir', type=str, default='./evaluation', help='Directory to save outputs')\n",
    "    parser.add_argument('--num_samples_to_save', type=int, default=5, help='Number of sample batches to save')\n",
    "    \n",
    "    # Use parse_known_args() to ignore unknown arguments passed by Jupyter\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    evaluate(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.1990\n",
    "#0.0937\n",
    "# 1.055\n",
    "#0.1998\n",
    "#0.2609\n",
    "#0.3249\n",
    "#0.2775\n",
    "#0.3084\n",
    "#0.3306\n",
    "#0.3503 \n",
    "#0.3680\n",
    "\n",
    "#0.3959\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
